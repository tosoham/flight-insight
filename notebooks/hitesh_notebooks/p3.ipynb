{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f8cc996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Data Manipulation & Analysis\n",
    "# ------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------\n",
    "# Visualization\n",
    "# ------------------------------\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ------------------------------\n",
    "# Machine Learning\n",
    "# ------------------------------\n",
    "# Core\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    accuracy_score\n",
    ")\n",
    "from sklearn.base import clone\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "# Class Imbalance\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# ------------------------------\n",
    "# System Utilities\n",
    "# ------------------------------\n",
    "import datetime\n",
    "import warnings\n",
    "import gc\n",
    "import psutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ------------------------------\n",
    "# Configuration\n",
    "# ------------------------------\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf4c860",
   "metadata": {},
   "source": [
    "# STEP 1 - loading and merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5874a481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2w/hy3qhkf90ld86v626flq0kb80000gn/T/ipykernel_3012/3384630006.py:1: DtypeWarning: Columns (7,8) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  flights = pd.read_csv(\"archive/flights.csv\")\n"
     ]
    }
   ],
   "source": [
    "flights = pd.read_csv(\"archive/flights.csv\")\n",
    "airports = pd.read_csv('archive/airports.csv')\n",
    "airlines = pd.read_csv('archive/airlines.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ac88d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights[['AIR_SYSTEM_DELAY','SECURITY_DELAY','AIRLINE_DELAY','LATE_AIRCRAFT_DELAY','WEATHER_DELAY']] = flights[['AIR_SYSTEM_DELAY','SECURITY_DELAY','AIRLINE_DELAY','LATE_AIRCRAFT_DELAY','WEATHER_DELAY']].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02190efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# STEP 0: Join Airlines\n",
    "# -----------------------\n",
    "flights = flights.merge(\n",
    "    airlines,\n",
    "    left_on=\"AIRLINE\", right_on=\"IATA_CODE\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"IATA_CODE\"])  # drop duplicate airline code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c261b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# STEP 1: Join Airports (Origin)\n",
    "# -----------------------\n",
    "flights = flights.merge(\n",
    "    airports.add_prefix(\"ORG_\"),\n",
    "    left_on=\"ORIGIN_AIRPORT\", right_on=\"ORG_IATA_CODE\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"ORG_IATA_CODE\", \"ORG_AIRPORT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "25085b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# STEP 2: Join Airports (Destination)\n",
    "# -----------------------\n",
    "flights = flights.merge(\n",
    "    airports.add_prefix(\"DEST_\"),\n",
    "    left_on=\"DESTINATION_AIRPORT\", right_on=\"DEST_IATA_CODE\",\n",
    "    how=\"left\"\n",
    ").drop(columns=[\"DEST_IATA_CODE\", \"DESTINATION_AIRPORT\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4688fbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = flights.dropna(subset = ['ORG_LATITUDE','ORG_LONGITUDE','DEST_LATITUDE','DEST_LONGITUDE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5db9523",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['YEAR', 'MONTH', 'DAY', 'DAY_OF_WEEK', 'AIRLINE_x', 'FLIGHT_NUMBER',\n",
       "       'TAIL_NUMBER', 'ORIGIN_AIRPORT', 'SCHEDULED_DEPARTURE',\n",
       "       'DEPARTURE_TIME', 'DEPARTURE_DELAY', 'TAXI_OUT', 'WHEELS_OFF',\n",
       "       'SCHEDULED_TIME', 'ELAPSED_TIME', 'AIR_TIME', 'DISTANCE', 'WHEELS_ON',\n",
       "       'TAXI_IN', 'SCHEDULED_ARRIVAL', 'ARRIVAL_TIME', 'ARRIVAL_DELAY',\n",
       "       'DIVERTED', 'CANCELLED', 'CANCELLATION_REASON', 'AIR_SYSTEM_DELAY',\n",
       "       'SECURITY_DELAY', 'AIRLINE_DELAY', 'LATE_AIRCRAFT_DELAY',\n",
       "       'WEATHER_DELAY', 'AIRLINE_y', 'ORG_CITY', 'ORG_STATE', 'ORG_COUNTRY',\n",
       "       'ORG_LATITUDE', 'ORG_LONGITUDE', 'DEST_AIRPORT', 'DEST_CITY',\n",
       "       'DEST_STATE', 'DEST_COUNTRY', 'DEST_LATITUDE', 'DEST_LONGITUDE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c28c0ecd",
   "metadata": {},
   "source": [
    "# STEP 2 - preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00ddff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns:\n",
      " ['DAY_OF_WEEK', 'AIRLINE', 'ORIGIN_AIRPORT', 'SCHEDULED_TIME', 'DISTANCE', 'DEST_AIRPORT', 'DELAYED', 'DEPARTURE_HOUR', 'ARRIVAL_HOUR', 'MONTH', 'IS_WEEKEND', 'IS_HOLIDAY']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2w/hy3qhkf90ld86v626flq0kb80000gn/T/ipykernel_3012/4148073134.py:74: FutureWarning: The behavior of 'isin' with dtype=datetime64[ns] and castable values (e.g. strings) is deprecated. In a future version, these will not be considered matching by isin. Explicitly cast to the appropriate dtype before calling isin instead.\n",
      "  df[\"IS_HOLIDAY\"] = df[\"DATE\"].isin(us_holidays).astype(int)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compute mean arrival delay\n",
    "df=flights.copy()\n",
    "# mean_delay = df[\"ARRIVAL_DELAY\"].mean()\n",
    "mean_delay=15\n",
    "\n",
    "# Create target column using mean as threshold\n",
    "df[\"DELAYED\"] = (df[\"ARRIVAL_DELAY\"] >= mean_delay).astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 1: Create DATE column\n",
    "# -----------------------------\n",
    "df[\"DATE\"] = pd.to_datetime(df[[\"YEAR\", \"MONTH\", \"DAY\"]])\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 2: Extract time-based features\n",
    "# -----------------------------\n",
    "# Departure and Arrival hours (e.g., 1345 â†’ 13)\n",
    "df[\"DEPARTURE_HOUR\"] = (df[\"SCHEDULED_DEPARTURE\"] // 100).astype(int)\n",
    "df[\"ARRIVAL_HOUR\"] = (df[\"SCHEDULED_ARRIVAL\"] // 100).astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 3: Drop redundant columns\n",
    "# -----------------------------\n",
    "drop_cols = [\n",
    "    # Date parts (replaced by DATE + derived hours)\n",
    "    \"YEAR\", \"MONTH\", \"DAY\",\n",
    "    \"SCHEDULED_DEPARTURE\", \"SCHEDULED_ARRIVAL\",\n",
    "    \n",
    "    # Flight identifiers (too high cardinality)\n",
    "    \"FLIGHT_NUMBER\", \"TAIL_NUMBER\",\n",
    "    \n",
    "    # Duplicate airline column\n",
    "    \"AIRLINE_y\",\n",
    "    \n",
    "    # Redundant airport info (covered by airport codes)\n",
    "    \"ORG_CITY\", \"ORG_STATE\", \"ORG_COUNTRY\",\n",
    "    \"DEST_CITY\", \"DEST_STATE\", \"DEST_COUNTRY\",\n",
    "    \n",
    "    # Drop coordinates (we keep DISTANCE instead)\n",
    "    \"ORG_LATITUDE\", \"ORG_LONGITUDE\",\n",
    "    \"DEST_LATITUDE\", \"DEST_LONGITUDE\",\n",
    "    \n",
    "    # ðŸš« Post-flight / leakage features\n",
    "    \"DEPARTURE_TIME\", \"DEPARTURE_DELAY\",\n",
    "    \"TAXI_OUT\", \"TAXI_IN\", \"WHEELS_OFF\", \"WHEELS_ON\",\n",
    "    \"ELAPSED_TIME\", \"AIR_TIME\",\n",
    "    \"ARRIVAL_TIME\", \"ARRIVAL_DELAY\",  # target will be created separately\n",
    "    \"DIVERTED\", \"CANCELLED\", \"CANCELLATION_REASON\",\n",
    "    \"AIR_SYSTEM_DELAY\", \"SECURITY_DELAY\", \"AIRLINE_DELAY\",\n",
    "    \"LATE_AIRCRAFT_DELAY\", \"WEATHER_DELAY\"\n",
    "]\n",
    "\n",
    "\n",
    "df = df.drop(columns=[col for col in drop_cols if col in df.columns])\n",
    "\n",
    "# Correct way to rename column\n",
    "df = df.rename(columns={'AIRLINE_x': 'AIRLINE'})\n",
    "\n",
    "df = df.dropna(subset=['SCHEDULED_TIME'])\n",
    "#EXTRACT MORE FEATURES\n",
    "import pandas as pd\n",
    "import holidays\n",
    "\n",
    "# Extract month\n",
    "df[\"MONTH\"] = df[\"DATE\"].dt.month\n",
    "\n",
    "# Extract weekend (1 = weekend, 0 = weekday)\n",
    "df[\"IS_WEEKEND\"] = df[\"DATE\"].dt.dayofweek.isin([5, 6]).astype(int)\n",
    "\n",
    "# Extract holidays (example: US holidays, you can change to 'India' etc.)\n",
    "us_holidays = holidays.US(years=df[\"DATE\"].dt.year.unique())\n",
    "df[\"IS_HOLIDAY\"] = df[\"DATE\"].isin(us_holidays).astype(int)\n",
    "\n",
    "# Drop DATE itself (after feature engineering)\n",
    "df = df.drop(columns=[\"DATE\"])\n",
    "\n",
    "# -----------------------------\n",
    "# STEP 4: Verify remaining features\n",
    "# -----------------------------\n",
    "print(\"Remaining columns:\\n\", df.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e54730",
   "metadata": {},
   "source": [
    "1. Encode Categorical Features\n",
    "2. Scale Numerical Features\n",
    "3. Handle Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a280b765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class distribution before SMOTE:\n",
      "DELAYED\n",
      "0    0.811799\n",
      "1    0.188201\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_features = ['AIRLINE', 'ORIGIN_AIRPORT', 'DEST_AIRPORT']\n",
    "numerical_features = ['DAY_OF_WEEK', 'SCHEDULED_TIME', 'DISTANCE', \n",
    "                     'DEPARTURE_HOUR', 'ARRIVAL_HOUR', 'MONTH']\n",
    "\n",
    "# 1. Encode Categorical Features\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    label_encoders[col] = LabelEncoder()\n",
    "    df[col] = label_encoders[col].fit_transform(df[col])\n",
    "\n",
    "# 2. Scale Numerical Features\n",
    "scaler = StandardScaler()\n",
    "df[numerical_features] = scaler.fit_transform(df[numerical_features])\n",
    "\n",
    "# 3. Handle Class Imbalance\n",
    "# First, let's check class distribution\n",
    "print(\"Class distribution before SMOTE:\")\n",
    "print(df['DELAYED'].value_counts(normalize=True))\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('DELAYED', axis=1)\n",
    "y = df['DELAYED']\n",
    "# Apply SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Convert back to DataFrame\n",
    "df_balanced = pd.DataFrame(X_resampled, columns=X.columns)\n",
    "df_balanced['DELAYED'] = y_resampled\n",
    "\n",
    "print(\"\\nClass distribution after SMOTE:\")\n",
    "print(df_balanced['DELAYED'].value_counts(normalize=True))\n",
    "\n",
    "# Show the final shape of the balanced dataset\n",
    "print(\"\\nFinal dataset shape:\", df_balanced.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419f8f1",
   "metadata": {},
   "source": [
    "# STEP 3 - model implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81d2f8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "-------------------- Random Forest --------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m-\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Predict once (reuse later)\u001b[39;00m\n\u001b[32m     42\u001b[39m y_pred = model.predict(X_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vscode/coding/cog_npn/myenv/lib/python3.12/site-packages/sklearn/base.py:1365\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1358\u001b[39m     estimator._validate_params()\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1361\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1362\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1363\u001b[39m     )\n\u001b[32m   1364\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1365\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vscode/coding/cog_npn/myenv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:486\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    475\u001b[39m trees = [\n\u001b[32m    476\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    478\u001b[39m ]\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m486\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    508\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vscode/coding/cog_npn/myenv/lib/python3.12/site-packages/sklearn/utils/parallel.py:82\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     73\u001b[39m warning_filters = warnings.filters\n\u001b[32m     74\u001b[39m iterable_with_config_and_warning_filters = (\n\u001b[32m     75\u001b[39m     (\n\u001b[32m     76\u001b[39m         _with_config_and_warning_filters(delayed_func, config, warning_filters),\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     81\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config_and_warning_filters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vscode/coding/cog_npn/myenv/lib/python3.12/site-packages/joblib/parallel.py:1986\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1984\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._get_sequential_output(iterable)\n\u001b[32m   1985\u001b[39m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1986\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1988\u001b[39m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[32m   1989\u001b[39m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[32m   1990\u001b[39m \u001b[38;5;66;03m# reused, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[32m   1991\u001b[39m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[32m   1992\u001b[39m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[32m   1993\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vscode/coding/cog_npn/myenv/lib/python3.12/site-packages/joblib/parallel.py:1914\u001b[39m, in \u001b[36mParallel._get_sequential_output\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_batches += \u001b[32m1\u001b[39m\n\u001b[32m   1913\u001b[39m \u001b[38;5;28mself\u001b[39m.n_dispatched_tasks += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1914\u001b[39m res = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1915\u001b[39m \u001b[38;5;28mself\u001b[39m.n_completed_tasks += \u001b[32m1\u001b[39m\n\u001b[32m   1916\u001b[39m \u001b[38;5;28mself\u001b[39m.print_progress()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vscode/coding/cog_npn/myenv/lib/python3.12/site-packages/sklearn/utils/parallel.py:147\u001b[39m, in \u001b[36m_FuncWrapper.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(**config), warnings.catch_warnings():\n\u001b[32m    146\u001b[39m     warnings.filters = warning_filters\n\u001b[32m--> \u001b[39m\u001b[32m147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vscode/coding/cog_npn/myenv/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:188\u001b[39m, in \u001b[36m_parallel_build_trees\u001b[39m\u001b[34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m class_weight == \u001b[33m\"\u001b[39m\u001b[33mbalanced_subsample\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    186\u001b[39m         curr_sample_weight *= compute_sample_weight(\u001b[33m\"\u001b[39m\u001b[33mbalanced\u001b[39m\u001b[33m\"\u001b[39m, y, indices=indices)\n\u001b[32m--> \u001b[39m\u001b[32m188\u001b[39m     \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    189\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    190\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    196\u001b[39m     tree._fit(\n\u001b[32m    197\u001b[39m         X,\n\u001b[32m    198\u001b[39m         y,\n\u001b[32m   (...)\u001b[39m\u001b[32m    201\u001b[39m         missing_values_in_feature_mask=missing_values_in_feature_mask,\n\u001b[32m    202\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/vscode/coding/cog_npn/myenv/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Split dataset\n",
    "# ----------------------------\n",
    "X = df_balanced.drop('DELAYED', axis=1)\n",
    "y = df_balanced['DELAYED']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Initialize models\n",
    "# ----------------------------\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Train + Evaluate\n",
    "# ----------------------------\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{'-'*20} {name} {'-'*20}\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict once (reuse later)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Accuracy + classification report\n",
    "    print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Cross-validation score (parallelized, discard scores after printing)\n",
    "    cv_result = cross_val_score(model, X_train, y_train, cv=5, n_jobs=-1)\n",
    "    print(f\"\\nCross-validation accuracy: {np.mean(cv_result):.4f} (+/- {np.std(cv_result) * 2:.4f})\")\n",
    "    del cv_result  # free memory\n",
    "    \n",
    "    # Feature importance (tree models only)\n",
    "    if hasattr(model, 'feature_importances_'):\n",
    "        importances = zip(X_train.columns, model.feature_importances_)\n",
    "        top5 = sorted(importances, key=lambda x: x[1], reverse=True)[:5]\n",
    "        print(\"\\nTop 5 most important features:\")\n",
    "        for feature, score in top5:\n",
    "            print(f\"{feature}: {score:.4f}\")\n",
    "    \n",
    "    # Confusion Matrix (plotted one by one to save memory)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Free predictions + model after use\n",
    "    del y_pred\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1352a71c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
